{"cells":[{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"ename":"ImportError","evalue":"Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[1;32m/Users/dana/Desktop/dl project/mbert.ipynb Cell 1\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m model \u001b[39m=\u001b[39m BertForMaskedLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-base-multilingual-cased\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# Define training arguments\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     output_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./mbert-kazakh-normalizer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     eval_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     save_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m5e-5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# Set up the Trainer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dana/Desktop/dl%20project/mbert.ipynb#W1sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m )\n","File \u001b[0;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1772\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1770\u001b[0m \u001b[39m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1772\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m   1774\u001b[0m \u001b[39m# Disable average tokens when using single device\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maverage_tokens_across_devices:\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:2294\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2290\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2291\u001b[0m \u001b[39mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2292\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2293\u001b[0m requires_backends(\u001b[39mself\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m-> 2294\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_devices\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/utils/generic.py:62\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     60\u001b[0m cached \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, attr, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m cached \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget(obj)\n\u001b[1;32m     63\u001b[0m     \u001b[39msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m cached\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:2167\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2166\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2167\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m   2168\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[39m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[39m}\u001b[39;00m\u001b[39m`: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2169\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[39m'\u001b[39m\u001b[39maccelerate>=\u001b[39m\u001b[39m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2170\u001b[0m         )\n\u001b[1;32m   2171\u001b[0m \u001b[39m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2172\u001b[0m accelerator_state_kwargs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39menabled\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_configured_state\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mFalse\u001b[39;00m}\n","\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"]}],"source":["from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import pandas as pd\n","from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments\n","from torch.utils.data import Dataset\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","df = pd.read_csv(\"/Users/dana/Desktop/dl project/character_normalization_pairs_cleaned_output.csv\")\n","\n","# Split dataset\n","train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Load mBERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","\n","# Define a custom Dataset class\n","class NormalizationDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        input_text = item[\"input\"]\n","        reference_text = item[\"reference\"]\n","\n","        # Tokenize input and reference text at the character level\n","        input_encoding = self.tokenizer(input_text, return_tensors='pt', padding='max_length', truncation=True, max_length=32)\n","        reference_encoding = self.tokenizer(reference_text, return_tensors='pt', padding='max_length', truncation=True, max_length=32)\n","\n","        return {\n","            \"input_ids\": input_encoding[\"input_ids\"].squeeze(),\n","            \"attention_mask\": input_encoding[\"attention_mask\"].squeeze(),\n","            \"labels\": reference_encoding[\"input_ids\"].squeeze()\n","        }\n","\n","# Prepare the dataset and dataloaders\n","train_dataset = NormalizationDataset(train_data, tokenizer)\n","train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n","\n","# Load pre-trained mBERT model\n","model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./mbert-kazakh-normalizer\",\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=5e-5,\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n","\n","# Set up the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n",")\n","\n","# Start training\n","trainer.train()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":2}
